# 2022-03-30

- DONE Look at code from Chris Cueva
	- Code is written in MATLAB and well-documented
	- Code has a lot of flexibility: multiple nonlinearities and loss functions are supported
	- Hessian-free optimization is coded up manually (`conjgrad.m`)
	- Trajectory simulation code is in `generateINandTARGETOUT.m`
		- What's this? http://www.walkingrandomly.com/?p=2945
		- Angle and speed seem to be chosen similarly to what we're doing, with some slight differences at boundary (instead of uniformly sampling, they keep Gaussian sampling of angular velocity, but increase variance if a certain number of samples fail to get the animal back in boundary)
		- NOTE: Gaussian noise is added to input (was this mentioned in paper?)
	- RNN code is in `rnn.m`
		- Weight matrices are initialized more carefully than what we're doing
		- Look at orthogonal random matrix initialization from Saxe et al., 2014 -- this apparently speeds up training at same level as layer-wise pretraining
		- During training, they also use gradient clipping (Pascanu et al., 2013)
		- Training is very complicated! They seem to be using a damping parameter?
		- Question: Why are they computing certain steps on the entire training set? Didn't the paper say they were using minibatches?
