# 2022-05-12

- DONE Train RNN with $\lambda_h = 2.0$
	- Run finished. Loss clearly converged.
	- Performance is normal
		- ![20220511_01_performance.png](../assets/20220511_01_performance_1652551683222_0.png)
	- Tuning is the same
		- ![20220511_01_tuning.png](../assets/20220511_01_tuning_1652557420501_0.png)
	- Activity is even more sparse
		- ![20220511_01_activity.png](../assets/20220511_01_activity_1652557434634_0.png)
	- Need to run with even greater metabolic penalty
- TODO Add CUDA acceleration to RNN training code 
- TODO Add MSE trace to `Trainer` class 
- DONE Add network checkpoints to `Trainer` class 
- TODO Sanity check: Make sure I'm not using fewer data points than Ganguli code 
- TODO Sanity check: Test performance on single trajectory rotated 90 degrees four times 
	- This is to make sure theta is symmetrical
- TODO Add different boundary collision handling to code 
	- Direction angle continues Brownian motion process uninterrupted
	- If current speed and direction will take agent outside of boundary, speed is set to zero
- TODO Compute RNN performance as function of trajectory timestep and see if this correlates with boundary collisions (or smoothed version of them) 
